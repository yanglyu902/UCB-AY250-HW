{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aging-schema",
   "metadata": {},
   "source": [
    "# HW6: Machine learning\n",
    "\n",
    "**Due Wednesday April 6, 8pm**\n",
    "\n",
    "(AY250 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-median",
   "metadata": {},
   "source": [
    "## Galaxy Classification with CNNs\n",
    "\n",
    "Galaxies appear with different shapes and colors depending on a number of factors, including age, distance, and history of mergers with other galaxies. A crowd-sourced group was [asked to visually classify a set of galaxies from a telescope survey](https://data.galaxyzoo.org/gz_trees/gz_trees.html), resulting in a labelled dataset of 21,785 galaxies across 10 different labels.\n",
    "\n",
    "You can download it locally:\n",
    "```bash\n",
    "curl -o galaxies10.h5 http://astro.utoronto.ca/~bovy/Galaxy10/Galaxy10.h5\n",
    "```\n",
    "Or access it on the shared drive on `astro.datahub.berkeley.edu` at `shared/HW_6/galaxies10.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-consortium",
   "metadata": {},
   "source": [
    "First, load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# To get the images and labels from file\n",
    "with h5py.File('galaxies10.h5', 'r') as F:\n",
    "    images = np.array(F['images'])\n",
    "    labels = np.array(F['ans'])\n",
    "\n",
    "# Convert the labels to categorical 10 classes\n",
    "labels = utils.to_categorical(labels, 10)\n",
    "\n",
    "# Convert to desirable type\n",
    "labels = labels.astype(np.float32)\n",
    "images = images.astype(np.float32)\n",
    "\n",
    "print(f\"labels shape: {labels.shape}\")\n",
    "print(f\"images shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-thinking",
   "metadata": {},
   "source": [
    "### Visualize and inspect\n",
    "\n",
    "In a single, multi-panel figure visualize one randomly chosen galaxy from each of the 10 classes. Be sure to set the random seed so that your figure will be reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-edgar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-telescope",
   "metadata": {},
   "source": [
    "Show a histogram of the distribution of galaxies across classes. Comment on any differences you notice with this dataset and the MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-federal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-means",
   "metadata": {},
   "source": [
    "### Prepare for training\n",
    "\n",
    "First, split the dataset into a train, validation, and test set with a fixed random seed. Then, construct a simple (~few layers) CNN that will accommodate the 69x69x3 images and return a classification. Hint: you will need to modify the networks we built in class to handle the 3d inputs (check out `Conv3D` https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-delight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "embedded-sailing",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Using early stopping and learning rate decay, train the network you built. Use the validation dataset at the end of each epoch for the early stopping. You might decide to do this part on colab for GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-failure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corrected-torture",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Use your trained model and apply it to the test set. How well did you do (total accuracy)? Plot a confusion matrix and comment where the network was most confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-family",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eleven-naples",
   "metadata": {},
   "source": [
    "### Improve - data augmentation\n",
    "\n",
    "Now that you have a baseline accuracy, you can work to try to improve the results. Without changing your network architecture, implement a data augmentation strategy, retrain, and then reevaluate. How much improvement did you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-allergy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "configured-lesbian",
   "metadata": {},
   "source": [
    "### Improve - network\n",
    "\n",
    "Change/embellish your original network to try to improve the results (try adding layers, changing the initialization of the kernel weights, changing the activation, adding dropout). You can use an automl trainer if you'd like or try different approaches by hand. Retrain and then reevaluate. Explain what you did but only show us the best model and its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-tonight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surface-roman",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "A form of non-parametric representation learning with neutral nets, where the architecture of the network is used to reduce the dimensionality of the data. First, go through (and execute) the following codeblocks and make sure you understand what they are doing to build and use an autoencoder.\n",
    "\n",
    "As the name suggests, autoencoders uses the data itself to learn the best way to represent it in a compact way--it's a form of semantic compression. This is a family of self- (or un-) supervised modeling.\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\">\n",
    "Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n",
    "\n",
    "In practice, we take an input X (which may be a 1-d vector, 2-d image, ...) and try to squeeze it down to a smaller number of values in the \"bottleneck\" layer and then uncompress back to it's original shape and form. The loss function that we construct will be the way in which the network learns on each backprop through the data.\n",
    "\n",
    "Let's look at a autoencoder which uses convnets to restruct the fashion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, \\\n",
    "                                    Activation, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print keras version\n",
    "print(tensorflow.keras.__version__)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "nb_classes = 10\n",
    "batch_size = 128\n",
    "bottleneck_size = 64\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train =  to_categorical(y_train, nb_classes)\n",
    "Y_test =  to_categorical(y_test, nb_classes)\n",
    "\n",
    "input_shape = x_train[0].shape  + (1,)\n",
    "input_shape\n",
    "input_img = Input(shape = (28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-taylor",
   "metadata": {},
   "source": [
    "Make a simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Flatten()(x)\n",
    "bottleneck = Dense(bottleneck_size, name=\"bottleneck\")(x)\n",
    "\n",
    "x = Dense(128)(bottleneck)\n",
    "x = Reshape((4, 4, 8))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# add with tf.device('/gpu:0'): if on GPU\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "\n",
    "logdir = os.path.join(\n",
    "    \"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "\n",
    "model_path = f'nn_results/ay250_autoencoder_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode='min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "tensorboard_callback = \\\n",
    "    tensorflow.keras.callbacks.TensorBoard(logdir,\n",
    "                                           histogram_freq=0,\n",
    "                                           write_graph=True,\n",
    "                                           write_grads=False,\n",
    "                                           write_images=False,\n",
    "                                           embeddings_freq=0,\n",
    "                                           embeddings_layer_names=None,\n",
    "                                           embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "autoencoder_train = autoencoder.fit(x_train, x_train,\n",
    "                                    batch_size=batch_size, epochs=10,\n",
    "                                    verbose=1, shuffle=False,\n",
    "                                    validation_data=(x_test, x_test),\n",
    "                                    callbacks=[tensorboard_callback, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "latest_model_file = !ls -t1 nn_results/ay250_*.h5 | head -1\n",
    "latest_model_file = latest_model_file[0]\n",
    "print(f\"Using {latest_model_file}\")\n",
    "autoencoder = load_model(latest_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-decline",
   "metadata": {},
   "source": [
    "### Classify using Random Forest\n",
    "\n",
    "Using the autoencoder model above, create a random forest model to predict the classes of the images using the 64-parameter bottleneck layer. What accuracy do you get? How does it compare with the accuracy we got on the `convnet` model before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-replication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-deputy",
   "metadata": {},
   "source": [
    "### Bottleneck size variation\n",
    "\n",
    "Experiment with trying a different sized layer (e.g., size 4, 16, 32) and repeat step a) above. Do you see any trends with bottleneck size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-cooperation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9 AY250",
   "language": "python",
   "name": "ay250"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
